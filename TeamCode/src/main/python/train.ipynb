{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f88c3af-b329-4269-893b-07d0edf696b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import cv2 \n",
    "from random import shuffle\n",
    "import glob\n",
    "import tables\n",
    "import cv2\n",
    "from math import ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05066264-f88c-4cbe-b2c4-3161e0411c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad014cdd-af50-4ddc-874a-004b4ca526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileslist = [f for f in os.listdir(\"./blue/left\") if f.endswith('.jpg')]\n",
    "# img = Image.open('./blue/left/'+fileslist[0])\n",
    "\n",
    "# c = 100\n",
    "# for i in fileslist:\n",
    "#     img = Image.open('./blue/left/'+i)\n",
    "#     image_arr = np.array(img) \n",
    "#     # image_arr = cv2.GaussianBlur(image_arr, (101, 101), 0)\n",
    "#     image_1 = image_arr[0:320,0:480] \n",
    "#     image_2 = image_arr[0:320,480:960]\n",
    "#     image_3 = image_arr[0:320,960:1440]\n",
    "#     image_1 = Image.fromarray(image_1)\n",
    "#     image_2 = Image.fromarray(image_2)\n",
    "#     image_3 = Image.fromarray(image_3)\n",
    "#     image_1.save(\"./blue/train/cone/\"+str(c)+'.jpg')\n",
    "#     image_3.save(\"./blue/train/none/\"+str(c+1)+'.jpg')\n",
    "#     image_2.save(\"./blue/train/none/\"+str(c+2)+'.jpg')\n",
    "#     c+=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d781b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = './blue/dataset.hdf5'  # address to where you want to save the hdf5 file\n",
    "train_path = './blue/train/*/*.jpg' # path of the directory where image data is stored.\n",
    "data_order = 'tf' # different library uses different data ordering `tf` is used for tensor flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb98b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "addrs = glob.glob(train_path)\n",
    "labels = [0 if 'none' in addr else 1 for addr in addrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb35270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440cdca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 135\n",
      "val size: 45\n",
      "test size: 45\n"
     ]
    }
   ],
   "source": [
    "train_addrs = addrs[0:int(0.6*len(addrs))]\n",
    "train_labels = labels[0:int(0.6*len(labels))]\n",
    "\n",
    "val_addrs = addrs[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "val_labels = labels[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "\n",
    "test_addrs = addrs[int(0.8*len(addrs)):]\n",
    "test_labels = labels[int(0.8*len(labels)):]\n",
    "print('train size:',len(train_addrs))\n",
    "print('val size:',len(val_addrs))\n",
    "print('test size:',len(test_addrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a1941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10/135\n",
      "Train data: 20/135\n",
      "Train data: 30/135\n",
      "Train data: 40/135\n",
      "Train data: 50/135\n",
      "Train data: 60/135\n",
      "Train data: 70/135\n",
      "Train data: 80/135\n",
      "Train data: 90/135\n",
      "Train data: 100/135\n",
      "Train data: 110/135\n",
      "Train data: 120/135\n",
      "Train data: 130/135\n",
      "Validation data: 10/45\n",
      "Validation data: 20/45\n",
      "Validation data: 30/45\n",
      "Validation data: 40/45\n",
      "Test data: 10/45\n",
      "Test data: 20/45\n",
      "Test data: 30/45\n",
      "Test data: 40/45\n",
      "HDF5 Done\n",
      "In Finally\n"
     ]
    }
   ],
   "source": [
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "img_dtype = tables.UInt8Atom()  # dtype in which the images will be saved\n",
    "\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    data_shape = (0, 3, 256, 256)\n",
    "elif data_order == 'tf':\n",
    "    data_shape = (0, 256, 256, 3)\n",
    "\n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "try:\n",
    "    train_storage = hdf5_file.create_earray(hdf5_file.root, 'train_img', img_dtype, shape=data_shape)\n",
    "    val_storage = hdf5_file.create_earray(hdf5_file.root, 'val_img', img_dtype, shape=data_shape)\n",
    "    test_storage = hdf5_file.create_earray(hdf5_file.root, 'test_img', img_dtype, shape=data_shape)\n",
    "    \n",
    "    mean_storage = hdf5_file.create_earray(hdf5_file.root, 'train_mean', img_dtype, shape=data_shape)\n",
    "    \n",
    "    # create the label arrays and copy the labels data in them\n",
    "    hdf5_file.create_array(hdf5_file.root, 'train_labels', train_labels)\n",
    "    hdf5_file.create_array(hdf5_file.root, 'val_labels', val_labels)\n",
    "    hdf5_file.create_array(hdf5_file.root, 'test_labels', test_labels)\n",
    "    \n",
    "    # a numpy array to save the mean of the images\n",
    "    mean = np.zeros(data_shape[1:], np.float32)\n",
    "    \n",
    "    # loop over train addresses\n",
    "    for i in range(len(train_addrs)):\n",
    "        # print how many images are saved every 100 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "    \n",
    "        # read an image and resize to (224, 224)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = train_addrs[i]\n",
    "        #print(addr)\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image and calculate the mean so far\n",
    "        train_storage.append(img[None])\n",
    "        mean += img / float(len(train_labels))\n",
    "    \n",
    "    # loop over validation addresses\n",
    "    for i in range(len(val_addrs)):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print ('Validation data: {}/{}'.format(i, len(val_addrs)))\n",
    "    \n",
    "        # read an image and resize to (256, 256)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = val_addrs[i]\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image\n",
    "        val_storage.append(img[None])\n",
    "    \n",
    "    # loop over test addresses\n",
    "    for i in range(len(test_addrs)):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print ('Test data: {}/{}'.format(i, len(test_addrs)))\n",
    "    \n",
    "        # read an image and resize to (224, 224)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = test_addrs[i]\n",
    "        #print(addr)\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image\n",
    "        test_storage.append(img[None])\n",
    "    \n",
    "    # save the mean and close the hdf5 file\n",
    "    mean_storage.append(mean[None])\n",
    "    print('HDF5 Done')\n",
    "finally:\n",
    "    print('In Finally')\n",
    "    hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = './blue/dataset.hdf5'  # Path where dataset.hdf5 file is stored\n",
    "subtract_mean = True\n",
    "batch_size = 50\n",
    "nb_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (135, 256, 256, 3)  train_label (135,)\n",
      "test_data: (45, 256, 256, 3)  test_label: (45,)\n",
      "val_data: (45, 256, 256, 3)  val_label: (45,)\n"
     ]
    }
   ],
   "source": [
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file.root.train_mean[0]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "\n",
    "# Total number of samples\n",
    "train_data = np.array(hdf5_file.root.train_img)\n",
    "train_label = np.array(hdf5_file.root.train_labels)\n",
    "\n",
    "test_data = np.array(hdf5_file.root.test_img)\n",
    "test_label = np.array(hdf5_file.root.test_labels)\n",
    "\n",
    "val_data = np.array(hdf5_file.root.val_img)\n",
    "val_label = np.array(hdf5_file.root.val_labels)\n",
    "\n",
    "print('train data:',train_data.shape,' train_label',train_label.shape)\n",
    "print('test_data:',test_data.shape,' test_label:',test_label.shape)\n",
    "print('val_data:',val_data.shape,' val_label:',val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ec253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "(135, 2) train samples\n",
      "(45, 2) test samples\n",
      "(45, 2) validation samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "num_classes = len(np.unique(train_label))\n",
    "train_label = to_categorical(train_label, num_classes)\n",
    "test_label = to_categorical(test_label, num_classes)\n",
    "val_label = to_categorical(val_label, num_classes)\n",
    "\n",
    "# print shape of training set\n",
    "print('num_classes:', num_classes)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(train_label.shape, 'train samples')\n",
    "print(test_label.shape, 'test samples')\n",
    "print(val_label.shape, 'validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4983f-feee-4e35-9a3b-1cfd5efdcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 256, 256, 16)      208       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 128, 128, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 32)      2080      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 64, 64, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               67109376  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67112690 (256.01 MB)\n",
      "Trainable params: 67112690 (256.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(256, 256, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='tanh'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='tanh'))\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b098f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 127, 127, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 62, 62, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 230400)            0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                14745664  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14802114 (56.47 MB)\n",
      "Trainable params: 14802114 (56.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='selu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55070b0c-a4a8-4295-b417-07098c622620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997789e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 90.3616 - accuracy: 0.6148\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68889, saving model to model.weights.best_200.hdf5\n",
      "5/5 [==============================] - 6s 1s/step - loss: 90.3616 - accuracy: 0.6148 - val_loss: 20.6461 - val_accuracy: 0.6889\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.8071 - accuracy: 0.7704\n",
      "Epoch 2: val_accuracy improved from 0.68889 to 0.97778, saving model to model.weights.best_200.hdf5\n",
      "5/5 [==============================] - 10s 2s/step - loss: 7.8071 - accuracy: 0.7704 - val_loss: 0.0410 - val_accuracy: 0.9778\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8667\n",
      "Epoch 3: val_accuracy improved from 0.97778 to 1.00000, saving model to model.weights.best_200.hdf5\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.4298 - accuracy: 0.8667 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9407\n",
      "Epoch 4: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.1877 - accuracy: 0.9407 - val_loss: 1.6149 - val_accuracy: 0.7556\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.8963\n",
      "Epoch 5: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6227 - accuracy: 0.8963 - val_loss: 4.9037e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9333\n",
      "Epoch 6: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 1s/step - loss: 0.2658 - accuracy: 0.9333 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9481\n",
      "Epoch 7: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1953 - accuracy: 0.9481 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9111\n",
      "Epoch 8: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2314 - accuracy: 0.9111 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8741\n",
      "Epoch 9: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.4202 - accuracy: 0.8741 - val_loss: 4.5497e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9556\n",
      "Epoch 10: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0983 - accuracy: 0.9556 - val_loss: 1.8635e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8889\n",
      "Epoch 11: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.3495 - accuracy: 0.8889 - val_loss: 1.1202e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9852\n",
      "Epoch 12: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0476 - accuracy: 0.9852 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.8608 - accuracy: 0.8889\n",
      "Epoch 13: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 1.8608 - accuracy: 0.8889 - val_loss: 5.8932e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 25.8633 - accuracy: 0.7556\n",
      "Epoch 14: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 25.8633 - accuracy: 0.7556 - val_loss: 0.6937 - val_accuracy: 0.9778\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.9333\n",
      "Epoch 15: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4183 - accuracy: 0.9333 - val_loss: 7.0817e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9259\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.3235 - accuracy: 0.9259 - val_loss: 3.3220e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.9556\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.3496 - accuracy: 0.9556 - val_loss: 0.1194 - val_accuracy: 0.9778\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9704\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0695 - accuracy: 0.9704 - val_loss: 9.2718e-08 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9778\n",
      "Epoch 19: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0610 - accuracy: 0.9778 - val_loss: 3.6027e-07 - val_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9481\n",
      "Epoch 20: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.2040 - accuracy: 0.9481 - val_loss: 5.6160e-07 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9926\n",
      "Epoch 21: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0200 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 22: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9630\n",
      "Epoch 23: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0474 - accuracy: 0.9630 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9704\n",
      "Epoch 24: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2709 - accuracy: 0.9704 - val_loss: 1.0596e-08 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9852\n",
      "Epoch 25: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0389 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
      "Epoch 26: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9852\n",
      "Epoch 27: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0296 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3335e-04 - accuracy: 1.0000\n",
      "Epoch 28: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 1s/step - loss: 1.3335e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9926\n",
      "Epoch 29: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0379 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.9744e-05 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.9744e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9926  \n",
      "Epoch 31: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0399 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.2981e-07 - accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 1.2981e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.9843e-06 - accuracy: 1.0000\n",
      "Epoch 33: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.9843e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2103e-05 - accuracy: 1.0000\n",
      "Epoch 34: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 3.2103e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9926\n",
      "Epoch 35: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0124 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3166e-04 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 1.3166e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9926  \n",
      "Epoch 37: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 940ms/step - loss: 0.0664 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.9630\n",
      "Epoch 38: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 970ms/step - loss: 0.3003 - accuracy: 0.9630 - val_loss: 0.2559 - val_accuracy: 0.9778\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9778\n",
      "Epoch 39: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 976ms/step - loss: 0.3035 - accuracy: 0.9778 - val_loss: 4.4315e-06 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9852\n",
      "Epoch 40: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0787 - accuracy: 0.9852 - val_loss: 6.4470e-06 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9704\n",
      "Epoch 41: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 937ms/step - loss: 0.0844 - accuracy: 0.9704 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9926\n",
      "Epoch 42: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 951ms/step - loss: 0.0111 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.5133e-06 - accuracy: 1.0000\n",
      "Epoch 43: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 939ms/step - loss: 7.5133e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9926\n",
      "Epoch 44: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 922ms/step - loss: 0.0319 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9926  \n",
      "Epoch 45: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 964ms/step - loss: 0.0118 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.2153e-06 - accuracy: 1.0000\n",
      "Epoch 46: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 5.2153e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9926\n",
      "Epoch 47: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0125 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 48: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 924ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.4072e-04 - accuracy: 1.0000\n",
      "Epoch 49: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 903ms/step - loss: 1.4072e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.4989e-07 - accuracy: 1.0000\n",
      "Epoch 50: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 929ms/step - loss: 2.4989e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000  \n",
      "Epoch 51: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 933ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.7210e-06 - accuracy: 1.0000\n",
      "Epoch 52: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 4.7210e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9926  \n",
      "Epoch 53: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 903ms/step - loss: 0.0098 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9926\n",
      "Epoch 54: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0284 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9852\n",
      "Epoch 55: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 951ms/step - loss: 0.0336 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.0364e-05 - accuracy: 1.0000\n",
      "Epoch 56: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 972ms/step - loss: 6.0364e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9926\n",
      "Epoch 57: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 957ms/step - loss: 0.0175 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9926\n",
      "Epoch 58: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 939ms/step - loss: 0.0374 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.3307e-06 - accuracy: 1.0000\n",
      "Epoch 59: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 945ms/step - loss: 5.3307e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9926\n",
      "Epoch 60: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 997ms/step - loss: 0.0213 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9926\n",
      "Epoch 61: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 986ms/step - loss: 0.0166 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.9926\n",
      "Epoch 62: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 961ms/step - loss: 0.4945 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 63: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 997ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9852\n",
      "Epoch 64: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 994ms/step - loss: 0.0405 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.5321e-08 - accuracy: 1.0000\n",
      "Epoch 65: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 3.5321e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9926\n",
      "Epoch 66: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0486 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9926\n",
      "Epoch 67: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0208 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.1165e-05 - accuracy: 1.0000\n",
      "Epoch 68: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 5.1165e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.3588e-05 - accuracy: 1.0000\n",
      "Epoch 69: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 976ms/step - loss: 2.3588e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.5610e-05 - accuracy: 1.0000\n",
      "Epoch 70: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 990ms/step - loss: 5.5610e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.1594e-05 - accuracy: 1.0000\n",
      "Epoch 71: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 982ms/step - loss: 1.1594e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9926\n",
      "Epoch 72: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 936ms/step - loss: 0.2757 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9852\n",
      "Epoch 73: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 965ms/step - loss: 0.1311 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9926  \n",
      "Epoch 74: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 912ms/step - loss: 0.0117 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 75: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 929ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.9607e-05 - accuracy: 1.0000\n",
      "Epoch 76: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 988ms/step - loss: 6.9607e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9852\n",
      "Epoch 77: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 948ms/step - loss: 0.0513 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9852\n",
      "Epoch 78: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 983ms/step - loss: 0.0188 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 79: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 80: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 988ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.0310e-04 - accuracy: 1.0000\n",
      "Epoch 81: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 956ms/step - loss: 2.0310e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3017 - accuracy: 0.9852  \n",
      "Epoch 82: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 939ms/step - loss: 1.3017 - accuracy: 0.9852 - val_loss: 0.3330 - val_accuracy: 0.9778\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3330 - val_accuracy: 0.9778\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9926  \n",
      "Epoch 84: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 976ms/step - loss: 0.0999 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.9852\n",
      "Epoch 85: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 927ms/step - loss: 0.3373 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000  \n",
      "Epoch 86: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 943ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 933ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.9543e-05 - accuracy: 1.0000\n",
      "Epoch 88: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 930ms/step - loss: 4.9543e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.2062e-05 - accuracy: 1.0000\n",
      "Epoch 89: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 944ms/step - loss: 1.2062e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 968ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 91: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 908ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9926\n",
      "Epoch 92: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0259 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 93: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 94: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 909ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 96: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 960ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9926\n",
      "Epoch 97: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 952ms/step - loss: 0.0091 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9926\n",
      "Epoch 98: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 939ms/step - loss: 0.0129 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 909ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.6195e-05 - accuracy: 1.0000\n",
      "Epoch 100: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 920ms/step - loss: 7.6195e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9926\n",
      "Epoch 101: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 892ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.4659e-06 - accuracy: 1.0000\n",
      "Epoch 102: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 918ms/step - loss: 4.4659e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.3130e-04 - accuracy: 1.0000\n",
      "Epoch 103: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 886ms/step - loss: 4.3130e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 104: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 913ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.7661e-09 - accuracy: 1.0000\n",
      "Epoch 105: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 894ms/step - loss: 1.7661e-09 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000  \n",
      "Epoch 106: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 903ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.4151e-08 - accuracy: 1.0000\n",
      "Epoch 107: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 925ms/step - loss: 4.4151e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7554e-05 - accuracy: 1.0000\n",
      "Epoch 108: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 919ms/step - loss: 3.7554e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9852  \n",
      "Epoch 109: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 899ms/step - loss: 0.2339 - accuracy: 0.9852 - val_loss: 7.6824e-08 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.9407\n",
      "Epoch 110: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 921ms/step - loss: 0.2553 - accuracy: 0.9407 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 111: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 906ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.9481\n",
      "Epoch 112: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 909ms/step - loss: 0.4652 - accuracy: 0.9481 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 113: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 919ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9926\n",
      "Epoch 114: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 904ms/step - loss: 0.0167 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 115: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 913ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 116: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 938ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0474e-05 - accuracy: 1.0000\n",
      "Epoch 117: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 906ms/step - loss: 1.0474e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.1757e-05 - accuracy: 1.0000\n",
      "Epoch 118: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 934ms/step - loss: 5.1757e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.0059e-04 - accuracy: 1.0000\n",
      "Epoch 119: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 933ms/step - loss: 8.0059e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 120: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 944ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 121: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 920ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 122: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 913ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9926  \n",
      "Epoch 123: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 929ms/step - loss: 0.0092 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 124: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 935ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9926\n",
      "Epoch 125: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 916ms/step - loss: 0.0460 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 126: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 923ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.6783e-05 - accuracy: 1.0000\n",
      "Epoch 127: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 971ms/step - loss: 5.6783e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4435e-05 - accuracy: 1.0000\n",
      "Epoch 128: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 3.4435e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 129: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 13s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.0451e-05 - accuracy: 1.0000\n",
      "Epoch 130: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 8.0451e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 131: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 17s 4s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3785e-05 - accuracy: 1.0000\n",
      "Epoch 132: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.3785e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7343e-04 - accuracy: 1.0000\n",
      "Epoch 133: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 3.7343e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000  \n",
      "Epoch 134: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 135: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 136: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 16s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 137: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9926  \n",
      "Epoch 138: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.2737e-04 - accuracy: 1.0000\n",
      "Epoch 139: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 8.2737e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.8802e-04 - accuracy: 1.0000\n",
      "Epoch 140: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 10s 2s/step - loss: 4.8802e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 141: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.0888e-04 - accuracy: 1.0000\n",
      "Epoch 142: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 21s 4s/step - loss: 5.0888e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 143: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.2247e-04 - accuracy: 1.0000\n",
      "Epoch 144: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 6.2247e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.0682e-05 - accuracy: 1.0000\n",
      "Epoch 145: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 12s 3s/step - loss: 4.0682e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.3497e-04 - accuracy: 1.0000\n",
      "Epoch 146: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 1s/step - loss: 8.3497e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.7209e-04 - accuracy: 1.0000\n",
      "Epoch 147: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 4.7209e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9926  \n",
      "Epoch 148: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0320 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.2872 - accuracy: 0.9185\n",
      "Epoch 149: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 6.2872 - accuracy: 0.9185 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9926\n",
      "Epoch 150: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0376 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.6440e-06 - accuracy: 1.0000\n",
      "Epoch 151: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 1.6440e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9852\n",
      "Epoch 152: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0371 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9926\n",
      "Epoch 153: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0310 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9852\n",
      "Epoch 154: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0145 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 155: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9926\n",
      "Epoch 156: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0142 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 157: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9926\n",
      "Epoch 158: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0069 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 159: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 160: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 161: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9926\n",
      "Epoch 162: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0188 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9926    \n",
      "Epoch 163: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 14s 3s/step - loss: 0.0054 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 164: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 944ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9926\n",
      "Epoch 165: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 917ms/step - loss: 0.0087 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.8139e-04 - accuracy: 1.0000\n",
      "Epoch 166: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 936ms/step - loss: 6.8139e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 167: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 884ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 168: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 873ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 169: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 931ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9778\n",
      "Epoch 170: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 887ms/step - loss: 0.1890 - accuracy: 0.9778 - val_loss: 0.5249 - val_accuracy: 0.9778\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.0963 - accuracy: 0.9481\n",
      "Epoch 171: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 930ms/step - loss: 2.0963 - accuracy: 0.9481 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.9852\n",
      "Epoch 172: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 878ms/step - loss: 0.3564 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 173: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 884ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 174: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 884ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.9926  \n",
      "Epoch 175: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 884ms/step - loss: 0.2643 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 176: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 877ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 177: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 874ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9926\n",
      "Epoch 178: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 885ms/step - loss: 0.0111 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9926\n",
      "Epoch 179: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 948ms/step - loss: 0.0100 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.6161e-05 - accuracy: 1.0000\n",
      "Epoch 180: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 911ms/step - loss: 1.6161e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.3716e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 870ms/step - loss: 2.3716e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 182: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 886ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.9037e-04 - accuracy: 1.0000\n",
      "Epoch 183: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 875ms/step - loss: 6.9037e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2723e-05 - accuracy: 1.0000\n",
      "Epoch 184: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 4s 873ms/step - loss: 3.2723e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.2694e-04 - accuracy: 1.0000\n",
      "Epoch 185: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 884ms/step - loss: 1.2694e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2888e-05 - accuracy: 1.0000\n",
      "Epoch 186: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 895ms/step - loss: 3.2888e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.8457 - accuracy: 0.9630\n",
      "Epoch 187: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 904ms/step - loss: 2.8457 - accuracy: 0.9630 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.4438 - accuracy: 0.9852\n",
      "Epoch 188: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 875ms/step - loss: 1.4438 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 189: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 899ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9926\n",
      "Epoch 190: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 904ms/step - loss: 0.0060 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9926\n",
      "Epoch 191: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 907ms/step - loss: 0.0913 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.3297e-05 - accuracy: 1.0000\n",
      "Epoch 192: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 886ms/step - loss: 8.3297e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 193: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 978ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9926\n",
      "Epoch 194: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.2130e-04 - accuracy: 1.0000\n",
      "Epoch 195: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.2130e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 196: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7466e-06 - accuracy: 1.0000\n",
      "Epoch 197: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 945ms/step - loss: 3.7466e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 198: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 954ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9926  \n",
      "Epoch 199: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0074 - accuracy: 0.9926 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 200: val_accuracy did not improve from 1.00000\n",
      "5/5 [==============================] - 5s 960ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best_200.hdf5', verbose=1, \n",
    "                               save_best_only=True,monitor='val_accuracy', mode = \"max\")\n",
    "hist = model.fit(train_data, train_label, batch_size=None, epochs=200,\n",
    "          validation_data=(val_data, val_label),callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b21f2-5426-4bfa-85e3-b6ee3c2a76c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"output_report.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa56a8f-b273-4b9f-aea8-a842c980552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.weights.best.hdf5')\n",
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d05428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1. 0.]\n",
      "tf.Tensor([0.7310586  0.26894143], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "a = model.predict(np.expand_dims(test_data[x], axis=0))\n",
    "print(a[0])\n",
    "score = tf.nn.softmax(a[0])\n",
    "print(score)\n",
    "img = Image.fromarray(test_data[x])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6df21-e3ca-49df-8ec7-792bc73778f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "[0. 1.]\n",
      "tf.Tensor([0.26894143 0.7310586 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "a = model.predict(np.expand_dims(test_data[x], axis=0))\n",
    "print(a[0])\n",
    "score = tf.nn.softmax(a[0])\n",
    "print(score)\n",
    "img = Image.fromarray(test_data[x])\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
