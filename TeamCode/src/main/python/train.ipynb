{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7cd95f-5465-44d8-b488-fd7effab60a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: matplotlib in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000015FC37DEE60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/tables/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000015FC37DF160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/tables/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000015FC37DF490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/tables/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000015FC37DF640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/tables/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000015FC37DF7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/tables/\n",
      "ERROR: Could not find a version that satisfies the requirement tables (from versions: none)\n",
      "ERROR: No matching distribution found for tables\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (10.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\25wan\\github\\centerstage-qualifier2\\venv\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow[and-cuda]\n",
    "# !pip install matplotlib\n",
    "# !pip install tables\n",
    "# !pip install pillow\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f88c3af-b329-4269-893b-07d0edf696b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.786110Z",
     "start_time": "2024-02-27T01:59:34.773614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\25wan\\GitHub\\CenterStage-Qualifier2\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtables\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tables'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import cv2 \n",
    "from random import shuffle\n",
    "import glob\n",
    "import tables\n",
    "import cv2\n",
    "from math import ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05066264-f88c-4cbe-b2c4-3161e0411c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.802982Z",
     "start_time": "2024-02-27T01:59:34.791708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad014cdd-af50-4ddc-874a-004b4ca526e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.845832Z",
     "start_time": "2024-02-27T01:59:34.842491Z"
    }
   },
   "outputs": [],
   "source": [
    "# fileslist = [f for f in os.listdir(\"./blue/left\") if f.endswith('.jpg')]\n",
    "# img = Image.open('./blue/left/'+fileslist[0])\n",
    "\n",
    "# c = 100\n",
    "# for i in fileslist:\n",
    "#     img = Image.open('./blue/left/'+i)\n",
    "#     image_arr = np.array(img) \n",
    "#     # image_arr = cv2.GaussianBlur(image_arr, (101, 101), 0)\n",
    "#     image_1 = image_arr[0:320,0:480] \n",
    "#     image_2 = image_arr[0:320,480:960]\n",
    "#     image_3 = image_arr[0:320,960:1440]\n",
    "#     image_1 = Image.fromarray(image_1)\n",
    "#     image_2 = Image.fromarray(image_2)\n",
    "#     image_3 = Image.fromarray(image_3)\n",
    "#     image_1.save(\"./blue/train/cone/\"+str(c)+'.jpg')\n",
    "#     image_3.save(\"./blue/train/none/\"+str(c+1)+'.jpg')\n",
    "#     image_2.save(\"./blue/train/none/\"+str(c+2)+'.jpg')\n",
    "#     c+=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d781b20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.869021Z",
     "start_time": "2024-02-27T01:59:34.865891Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = './blue/dataset.hdf5'  # address to where you want to save the hdf5 file\n",
    "train_path = './blue/train/*/*.jpg' # path of the directory where image data is stored.\n",
    "data_order = 'tf' # different library uses different data ordering `tf` is used for tensor flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb98b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.937097Z",
     "start_time": "2024-02-27T01:59:34.900004Z"
    }
   },
   "outputs": [],
   "source": [
    "addrs = glob.glob(train_path)\n",
    "labels = [0 if 'none' in addr else 1 for addr in addrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb35270a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.939147Z",
     "start_time": "2024-02-27T01:59:34.938658Z"
    }
   },
   "outputs": [],
   "source": [
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440cdca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.940412Z",
     "start_time": "2024-02-27T01:59:34.940267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 135\n",
      "val size: 45\n",
      "test size: 45\n"
     ]
    }
   ],
   "source": [
    "train_addrs = addrs[0:int(0.6*len(addrs))]\n",
    "train_labels = labels[0:int(0.6*len(labels))]\n",
    "\n",
    "val_addrs = addrs[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "val_labels = labels[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "\n",
    "test_addrs = addrs[int(0.8*len(addrs)):]\n",
    "test_labels = labels[int(0.8*len(labels)):]\n",
    "print('train size:',len(train_addrs))\n",
    "print('val size:',len(val_addrs))\n",
    "print('test size:',len(test_addrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4a1941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:34.977841Z",
     "start_time": "2024-02-27T01:59:34.952763Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 'th' for Theano, 'tf' for Tensorflow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241m.\u001b[39mUInt8Atom()  \u001b[38;5;66;03m# dtype in which the images will be saved\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# check the order of data and chose proper data shape to save images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tables' is not defined"
     ]
    }
   ],
   "source": [
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "img_dtype = tables.UInt8Atom()  # dtype in which the images will be saved\n",
    "\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    data_shape = (0, 3, 256, 256)\n",
    "elif data_order == 'tf':\n",
    "    data_shape = (0, 256, 256, 3)\n",
    "\n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "try:\n",
    "    train_storage = hdf5_file.create_earray(hdf5_file.root, 'train_img', img_dtype, shape=data_shape)\n",
    "    val_storage = hdf5_file.create_earray(hdf5_file.root, 'val_img', img_dtype, shape=data_shape)\n",
    "    test_storage = hdf5_file.create_earray(hdf5_file.root, 'test_img', img_dtype, shape=data_shape)\n",
    "    \n",
    "    mean_storage = hdf5_file.create_earray(hdf5_file.root, 'train_mean', img_dtype, shape=data_shape)\n",
    "    \n",
    "    # create the label arrays and copy the labels data in them\n",
    "    hdf5_file.create_array(hdf5_file.root, 'train_labels', train_labels)\n",
    "    hdf5_file.create_array(hdf5_file.root, 'val_labels', val_labels)\n",
    "    hdf5_file.create_array(hdf5_file.root, 'test_labels', test_labels)\n",
    "    \n",
    "    # a numpy array to save the mean of the images\n",
    "    mean = np.zeros(data_shape[1:], np.float32)\n",
    "    \n",
    "    # loop over train addresses\n",
    "    for i in range(len(train_addrs)):\n",
    "        # print how many images are saved every 100 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "    \n",
    "        # read an image and resize to (224, 224)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = train_addrs[i]\n",
    "        #print(addr)\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image and calculate the mean so far\n",
    "        train_storage.append(img[None])\n",
    "        mean += img / float(len(train_labels))\n",
    "    \n",
    "    # loop over validation addresses\n",
    "    for i in range(len(val_addrs)):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print ('Validation data: {}/{}'.format(i, len(val_addrs)))\n",
    "    \n",
    "        # read an image and resize to (256, 256)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = val_addrs[i]\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image\n",
    "        val_storage.append(img[None])\n",
    "    \n",
    "    # loop over test addresses\n",
    "    for i in range(len(test_addrs)):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if i % 10 == 0 and i > 1:\n",
    "            print ('Test data: {}/{}'.format(i, len(test_addrs)))\n",
    "    \n",
    "        # read an image and resize to (224, 224)\n",
    "        # cv2 load images as BGR, convert it to RGB\n",
    "        addr = test_addrs[i]\n",
    "        #print(addr)\n",
    "        img = cv2.imread(addr)\n",
    "        img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # add any image pre-processing here\n",
    "    \n",
    "        # if the data order is Theano, axis orders should change\n",
    "        if data_order == 'th':\n",
    "            img = np.rollaxis(img, 2)\n",
    "    \n",
    "        # save the image\n",
    "        test_storage.append(img[None])\n",
    "    \n",
    "    # save the mean and close the hdf5 file\n",
    "    mean_storage.append(mean[None])\n",
    "    print('HDF5 Done')\n",
    "finally:\n",
    "    print('In Finally')\n",
    "    hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = './blue/dataset.hdf5'  # Path where dataset.hdf5 file is stored\n",
    "subtract_mean = True\n",
    "batch_size = 50\n",
    "nb_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file.root.train_mean[0]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "\n",
    "# Total number of samples\n",
    "train_data = np.array(hdf5_file.root.train_img)\n",
    "train_label = np.array(hdf5_file.root.train_labels)\n",
    "\n",
    "test_data = np.array(hdf5_file.root.test_img)\n",
    "test_label = np.array(hdf5_file.root.test_labels)\n",
    "\n",
    "val_data = np.array(hdf5_file.root.val_img)\n",
    "val_label = np.array(hdf5_file.root.val_labels)\n",
    "\n",
    "print('train data:',train_data.shape,' train_label',train_label.shape)\n",
    "print('test_data:',test_data.shape,' test_label:',test_label.shape)\n",
    "print('val_data:',val_data.shape,' val_label:',val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ec253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "num_classes = len(np.unique(train_label))\n",
    "train_label = to_categorical(train_label, num_classes)\n",
    "test_label = to_categorical(test_label, num_classes)\n",
    "val_label = to_categorical(val_label, num_classes)\n",
    "\n",
    "# print shape of training set\n",
    "print('num_classes:', num_classes)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(train_label.shape, 'train samples')\n",
    "print(test_label.shape, 'test samples')\n",
    "print(val_label.shape, 'validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4983f-feee-4e35-9a3b-1cfd5efdcddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:35.034897Z",
     "start_time": "2024-02-27T01:59:35.020974Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                         input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(256, 256, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='tanh'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='tanh'))\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b098f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='selu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55070b0c-a4a8-4295-b417-07098c622620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:35.052081Z",
     "start_time": "2024-02-27T01:59:35.040815Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997789e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:35.092402Z",
     "start_time": "2024-02-27T01:59:35.075881Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint   \n\u001b[1;32m      3\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.weights.best_200.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_data, train_label, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      6\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(val_data, val_label),callbacks\u001b[38;5;241m=\u001b[39m[checkpointer], \n\u001b[1;32m      7\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best_200.hdf5', verbose=1, \n",
    "                               save_best_only=True,monitor='val_accuracy', mode = \"max\")\n",
    "hist = model.fit(train_data, train_label, batch_size=None, epochs=200,\n",
    "          validation_data=(val_data, val_label),callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b21f2-5426-4bfa-85e3-b6ee3c2a76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"output_report.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa56a8f-b273-4b9f-aea8-a842c980552c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:35.106611Z",
     "start_time": "2024-02-27T01:59:35.095598Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.weights.best.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_data, test_label, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, score[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.weights.best.hdf5')\n",
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d05428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386351f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T01:59:35.175150Z",
     "start_time": "2024-02-27T01:59:35.162344Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(test_data[x], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(a[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m score \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(a[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "a = model.predict(np.expand_dims(test_data[x], axis=0))\n",
    "print(a[0])\n",
    "score = tf.nn.softmax(a[0])\n",
    "print(score)\n",
    "img = Image.fromarray(test_data[x])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6df21-e3ca-49df-8ec7-792bc73778f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "a = model.predict(np.expand_dims(test_data[x], axis=0))\n",
    "print(a[0])\n",
    "score = tf.nn.softmax(a[0])\n",
    "print(score)\n",
    "img = Image.fromarray(test_data[x])\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
